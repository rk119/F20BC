{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hNd19Teur9Oi",
        "Gxmu6Hfs2ksg",
        "Tf7nIvVFBYbo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rk119/F20BC/blob/main/Copy_of_F20BC_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Biologically-Inspired Computation (F20BC) Coursework"
      ],
      "metadata": {
        "id": "BwtfpjfIbAEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview:**\n",
        "\n",
        "The following coursework aims to build an Artificial Neural Network (ANN) with Feedforward and instead of Backpropogation, the neural network is trained using the Particle Swarm Optimisation (PSO) algorithm, two biologically-inspired techniques which are taught in this course. The ANN structure and PSO algorithms are implemented from scratch, to build a deeper understanding as well as investigate or experiment with them flexibly to get a comprehensive idea on how the ANN is optimised using PSO in low level for a specified task.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZndbBMXblWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "cKTO__39Jhrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba88a48-336a-48d5-b03f-dc363492a7f6",
        "id": "vf_qrNCaJhr1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "wcNLhd4WJhr1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2d5mkrDUJhr2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "QpwwCL8vhC_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skipHeader = False\n",
        "normalize = True\n",
        "test_split = 0.2\n",
        "input_nodes = 4"
      ],
      "metadata": {
        "id": "Z6wyEz4thB8V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "Ilg00LwgJhr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if skipHeader:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/F20BC-Dataset/data_banknote_authentication.csv\", skiprows=1)\n",
        "else:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/F20BC-Dataset/data_banknote_authentication.csv\", header=None)"
      ],
      "metadata": {
        "id": "18mAqFikJhr3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View the first 5 instances of the data"
      ],
      "metadata": {
        "id": "Kg79sXWiJhr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a5aa0843-86db-4d1b-bc19-8d0fa8fb1f17",
        "id": "WK9Cik0QJhr4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0       1       2        3  4\n",
              "0  3.62160  8.6661 -2.8073 -0.44699  0\n",
              "1  4.54590  8.1674 -2.4586 -1.46210  0\n",
              "2  3.86600 -2.6383  1.9242  0.10645  0\n",
              "3  3.45660  9.5228 -4.0112 -3.59440  0\n",
              "4  0.32924 -4.4552  4.5718 -0.98880  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d4ac2d0-3897-46c3-9ef0-132aaac90248\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d4ac2d0-3897-46c3-9ef0-132aaac90248')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d4ac2d0-3897-46c3-9ef0-132aaac90248 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d4ac2d0-3897-46c3-9ef0-132aaac90248');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2162fe4-6b87-4e16-8085-7f39944add7c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2162fe4-6b87-4e16-8085-7f39944add7c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2162fe4-6b87-4e16-8085-7f39944add7c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the number of rows and columns"
      ],
      "metadata": {
        "id": "hyPQvWcRerXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127b926b-d668-48e7-ed22-ec75b311993f",
        "id": "j-kpRvxIJhr4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1372, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data into features and target\n",
        "\n",
        "**Note:** For one dataset, the target class is seperated into a different variable since we do not want the model to consider it as an input feature"
      ],
      "metadata": {
        "id": "uuHgHSume8AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_output(data):\n",
        "    X = data.iloc[:, :-1].to_numpy()\n",
        "    Y = data.iloc[:, -1].to_numpy()\n",
        "\n",
        "    return [X, Y]"
      ],
      "metadata": {
        "id": "o85EznF6Jhr5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = split_input_output(df)[0]\n",
        "y = split_input_output(df)[1]"
      ],
      "metadata": {
        "id": "iGBpMXqvfvAF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_vkc3mxjWnT",
        "outputId": "a2feeaf4-dad8-42e9-f2db-b7341e77553e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1372, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4_FOu6Qf63R",
        "outputId": "c23e7779-5fb5-46a2-d155-73328ddbc1f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n",
              "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n",
              "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n",
              "       ...,\n",
              "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n",
              "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n",
              "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939134fd-a7a1-48fb-b66f-dbab61239b03",
        "id": "xLeFBcvRJhr5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize if specified"
      ],
      "metadata": {
        "id": "8V6i5It5gPLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "gwjh6IFMJhr6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm = norm(x)\n",
        "x_norm"
      ],
      "metadata": {
        "outputId": "767058a0-352c-4937-cab4-83ec91aa5600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU0MO6hgJhr6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.76900389, 0.83964273, 0.10678269, 0.73662766],\n",
              "       [0.83565902, 0.82098209, 0.12180412, 0.64432563],\n",
              "       [0.78662859, 0.41664827, 0.31060805, 0.78695091],\n",
              "       ...,\n",
              "       [0.23738543, 0.01176814, 0.98560321, 0.52475518],\n",
              "       [0.25084193, 0.20170105, 0.76158701, 0.6606745 ],\n",
              "       [0.32452819, 0.49074676, 0.34334762, 0.88594888]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeR9c66vjDE9",
        "outputId": "cd2a5917-cd89-4866-9537-ca087667f905"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1372, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805fd801-3aaf-4640-c42a-6c90c83c8477",
        "id": "qwCyIh02Jhr7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1372"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c295db8f-64b5-41ce-bd9e-fbb80beeb05d",
        "id": "R3GSrU-MJhr7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1372"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into Train and Test"
      ],
      "metadata": {
        "id": "iY075IM8onTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm, x_test, y, y_test = train_test_split(x_norm, y, test_size=test_split, random_state=42)"
      ],
      "metadata": {
        "id": "wQG53vGhn_B6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network (ANN)\n",
        "\n",
        "Artificial Neural Networks (ANNs) are computational models inspired by the human brain. They are composed of interconnected units or nodes called artificial neurons, which simulate the way biological neurons signal each other. ANNs are used in machine learning for pattern recognition and data classification, among other tasks.\n",
        "\n",
        "In an ANN, neurons are organized in layers: an input layer to receive signals, one or more hidden layers to process them, and an output layer to deliver the final result. The network learns by adjusting the weights of connections based on the data it processes, typically through a method called backpropagation combined with a gradient descent optimization algorithm."
      ],
      "metadata": {
        "id": "bk9SgVfEh251"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to utilize later"
      ],
      "metadata": {
        "id": "mtF6z1SuJhr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Functions"
      ],
      "metadata": {
        "id": "Gxmu6Hfs2ksg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationFunctions:\n",
        "  def evaluate(self,x):\n",
        "    pass\n",
        "  def derivate(self,x):\n",
        "    pass\n",
        "\n",
        "class Identity:\n",
        "  def evaluate(self,x):\n",
        "    return x\n",
        "  def derivative(self,x):\n",
        "    return 1\n",
        "\n",
        "class Sigmoid(ActivationFunctions):\n",
        "  def evaluate(self,x):\n",
        "    x_clipped = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x_clipped))\n",
        "  def derivative(self,x):\n",
        "    f = self.evaluate(x)\n",
        "    return f * (1-f)\n",
        "\n",
        "class Tanh(ActivationFunctions):\n",
        "  def evaluate(self,x):\n",
        "    return np.tanh(x)\n",
        "  def derivative(self,x):\n",
        "    f = self.evaluate(x)\n",
        "    return 1 - f ** 2\n",
        "\n",
        "class relu(ActivationFunctions):\n",
        "  def evaluate(self,x):\n",
        "    return np.maximum(0, x)\n",
        "  def derivative(self,x):\n",
        "    return (x > 0).astype(float)"
      ],
      "metadata": {
        "id": "G1TURV8R1wdl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ],
      "metadata": {
        "id": "Tf7nIvVFBYbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunctions:\n",
        "  def evaluate(self,x):\n",
        "    pass\n",
        "  def derivate(self,x):\n",
        "    pass\n",
        "\n",
        "# y is predictions made by the neural network\n",
        "# t is target/actual numbers corresponding to inputs\n",
        "class Mse(LossFunctions):\n",
        "  def evaluate(self, y, t):\n",
        "    return ((t - y) ** 2).mean()\n",
        "  def derivative(self, y, t):\n",
        "    return 2 * (y - t) / len(y)\n",
        "\n",
        "class BinaryCrossEntropy(LossFunctions):\n",
        "  def evaluate(self, y, t):\n",
        "    y_pred = np.clip(y, 1e-7, 1 - 1e-7)\n",
        "    term0 = (1 - t) * np.log(1 - y_pred + 1e-7)\n",
        "    term1 = t * np.log(y_pred + 1e-7)\n",
        "    return - (term0 + term1).mean()\n",
        "\n",
        "  def derivative(self, y, t):\n",
        "    y_pred = np.clip(y, 1e-7, 1 - 1e-7)\n",
        "    return (t / y_pred) - (1 - t) / (1 - y_pred)\n",
        "\n",
        "class Hinge(LossFunctions):\n",
        "  def evaluate(self, y, t):\n",
        "    return np.maximum(0, 1 - t * y).mean()\n",
        "\n",
        "  def derivative(self, y, t):\n",
        "    return np.where(t * y < 1, -t, 0)"
      ],
      "metadata": {
        "id": "LQ_E70ULvdxe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "p77acdcFG12_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputLayer:\n",
        "    def __init__(self, input_size):\n",
        "        self.nb_nodes = input_size\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return input_data\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, nodes, activation, weights=None, biases=None):\n",
        "        self.nb_nodes = nodes\n",
        "        self.W = weights if weights is not None else np.random.randn(input_size, nodes)\n",
        "        self.B = biases if biases is not None else np.random.randn(nodes)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.X_in = input_data\n",
        "        z = np.dot(input_data, self.W) + self.B\n",
        "        out = self.activation.evaluate(z)\n",
        "        return out\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, configuration, position=None):\n",
        "        self.layers = []\n",
        "        input_size = configuration[0]\n",
        "\n",
        "        # The input layer is simply added as a pass-through layer\n",
        "        self.layers.append(InputLayer(input_size))\n",
        "\n",
        "        # If a position vector is provided, it contains weights and biases for each layer\n",
        "        if position is not None:\n",
        "            for idx, (nodes, activation) in enumerate(configuration[1:]):\n",
        "                weights, biases = position[idx]\n",
        "                # print(\"layer \", idx, \"W = \",weights, \"B = \", biases )\n",
        "                layer = Layer(input_size, nodes, activation, weights=weights, biases=biases)\n",
        "                self.add(layer)\n",
        "                input_size = nodes  # Update input size for the next layer\n",
        "        else:\n",
        "            # If no position vector, initialize layers with random weights and biases\n",
        "            for nodes, activation in configuration[1:]:\n",
        "                layer = Layer(input_size, nodes, activation)\n",
        "                self.add(layer)\n",
        "                input_size = nodes  # Update input size for the next layer\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def print_layers(self):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, InputLayer):\n",
        "                print(f\"Layer {i}: Input Layer with {layer.nb_nodes} nodes\")\n",
        "            elif isinstance(layer, Layer):\n",
        "                print(f\"Layer {i}: Hidden Layer with {layer.nb_nodes} nodes, Activation Function: {layer.activation.__class__.__name__}\")\n",
        "                print(f\"  Weights Shape: {layer.W.shape}, Biases Shape: {layer.B.shape}\")\n",
        "                print(f\"  Weights= : {layer.W}, Biases= {layer.B}\")\n",
        "            else:\n",
        "                print(f\"Layer {i}: Unknown Layer Type\")\n",
        "\n",
        "    def flatten_weights_and_biases(self):\n",
        "        flattened_vector = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Layer):\n",
        "                # Flatten and append weights and biases of this layer\n",
        "                flattened_weights = layer.W.flatten()\n",
        "                flattened_biases = layer.B.flatten()\n",
        "                flattened_vector.extend(flattened_weights.tolist())\n",
        "                flattened_vector.extend(flattened_biases.tolist())\n",
        "        return flattened_vector"
      ],
      "metadata": {
        "id": "WEK6G4PyG0tX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reconstructing Weights and Biases from Flattened Vector"
      ],
      "metadata": {
        "id": "G7bFem7gWZUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unflatten_weights_and_biases(flattened_vector, configuration):\n",
        "    position = []\n",
        "    index = 0\n",
        "\n",
        "    # Skip the input layer configuration, as it doesn't have weights or biases\n",
        "    for nodes, _ in configuration[1:]:\n",
        "        # Previous layer's node count is the number of rows for weights\n",
        "        input_size = configuration[0] if not position else position[-1][0].shape[1]\n",
        "\n",
        "        # Calculate the number of weights and biases\n",
        "        num_weights = input_size * nodes\n",
        "        num_biases = nodes\n",
        "\n",
        "        # Extract weights and biases from the flattened vector\n",
        "        weights = flattened_vector[index : index + num_weights]\n",
        "        biases = flattened_vector[index + num_weights : index + num_weights + num_biases]\n",
        "\n",
        "        # Reshape weights to the correct dimensions and add to position\n",
        "        position.append((np.array(weights).reshape(input_size, nodes), np.array(biases)))\n",
        "\n",
        "        # Update the index\n",
        "        index += num_weights + num_biases\n",
        "\n",
        "    return position"
      ],
      "metadata": {
        "id": "ARTTH2p-7XSg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating objects\n",
        "\n",
        "relu_activation = relu()\n",
        "sigmoid_activation = Sigmoid()\n",
        "tanh_activation = Tanh()"
      ],
      "metadata": {
        "id": "mu9Ux9u4Nafz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the layers with number of nodes, activation and the total number of elements here are the number of layers\n",
        "configuration = [\n",
        "    x_norm.shape[1],\n",
        "    [3, relu_activation],\n",
        "    [4, relu_activation],\n",
        "    [1, sigmoid_activation]\n",
        "]\n",
        "\n",
        "loss_function = BinaryCrossEntropy()"
      ],
      "metadata": {
        "id": "tEolcAKSNQR1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitness Function"
      ],
      "metadata": {
        "id": "JrL7yRFmWhAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness(X, Y, W):\n",
        "    # Rebuild the neural network with the given weights\n",
        "    position = unflatten_weights_and_biases(W, configuration)\n",
        "    # print(position)\n",
        "    neural_network = NeuralNetwork(configuration, position=position)\n",
        "\n",
        "    # Perform forward pass\n",
        "    predictions = neural_network.forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = loss_function.evaluate(predictions.flatten(), Y)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ZRO94sUvV8YX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Calculate Accuracy"
      ],
      "metadata": {
        "id": "tl9GTy-oWo5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calcualtes accuracy.\n",
        "    :param Y: int(N, )\n",
        "        Correct labels.\n",
        "    :param Y_pred: int(N, ) | double(N, C)\n",
        "        Predicted labels of shape(N, ) or (N, C) in case of one-hot vector.\n",
        "    :return: double\n",
        "        Accuracy.\n",
        "    \"\"\"\n",
        "    predicted_classes = (Y_pred >= 0.5).astype(int) # Convert probabilities to class labels\n",
        "    return (Y == predicted_classes).mean()"
      ],
      "metadata": {
        "id": "9fvQzmaKn4bG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempting the functions"
      ],
      "metadata": {
        "id": "vYqr2gulXBIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(configuration)"
      ],
      "metadata": {
        "id": "z8I6wfVAXLor"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.print_layers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-FPKUXOXN5o",
        "outputId": "5564c6cd-4578-493d-e18c-cffe7a08e9d6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: Input Layer with 4 nodes\n",
            "Layer 1: Hidden Layer with 3 nodes, Activation Function: relu\n",
            "  Weights Shape: (4, 3), Biases Shape: (3,)\n",
            "  Weights= : [[ 1.1301293  -1.97545545 -0.02691447]\n",
            " [ 0.19410106  0.13282557 -0.42301291]\n",
            " [ 1.27795215 -1.56041229 -2.06945538]\n",
            " [-0.5595845   0.33544935  0.54465029]], Biases= [0.64734701 0.9584159  1.25463   ]\n",
            "Layer 2: Hidden Layer with 4 nodes, Activation Function: relu\n",
            "  Weights Shape: (3, 4), Biases Shape: (4,)\n",
            "  Weights= : [[-0.94200426 -1.04157109 -1.53062947  0.84598513]\n",
            " [-0.67500584  0.15601773  0.63256895  0.02962299]\n",
            " [-2.06859649 -0.68539739  0.89980953  0.62085875]], Biases= [ 1.57415319 -0.5492156   1.00368003 -0.24174582]\n",
            "Layer 3: Hidden Layer with 1 nodes, Activation Function: Sigmoid\n",
            "  Weights Shape: (4, 1), Biases Shape: (1,)\n",
            "  Weights= : [[-0.58132126]\n",
            " [ 0.66402803]\n",
            " [-0.81661755]\n",
            " [ 0.05736805]], Biases= [-0.83959991]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_vector = nn.flatten_weights_and_biases()\n",
        "print(flattened_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "238MuHFHXQGU",
        "outputId": "26d198a7-0137-4729-998a-8b5e12caed78"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.130129297495017, -1.9754554455800695, -0.02691447431219012, 0.1941010584469044, 0.132825571107006, -0.4230129108652998, 1.2779521535338556, -1.5604122908841311, -2.069455379496961, -0.5595845011374354, 0.33544934883691496, 0.5446502924239798, 0.6473470104427345, 0.9584158963881246, 1.2546299999090356, -0.9420042604964283, -1.0415710911303782, -1.5306294657868524, 0.845985133577302, -0.6750058382607729, 0.15601772982951617, 0.6325689525799277, 0.02962299008638086, -2.0685964854084804, -0.6853973862561047, 0.8998095308361068, 0.6208587520013158, 1.5741531895544196, -0.5492155951342538, 1.0036800340756515, -0.2417458173122484, -0.5813212562618647, 0.6640280347440293, -0.816617552513038, 0.05736804829868501, -0.839599910248254]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = x_norm\n",
        "\n",
        "nn_init_pred = nn.forward(input_data)"
      ],
      "metadata": {
        "id": "mfqx5DHqXTDK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nn_init_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOPL_Q71XZFP",
        "outputId": "d5a9dc58-6cc9-420c-d706-caeefd986e1c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1097"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Particle Swarm Optimization (PSO)\n",
        "\n",
        "It is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, known as particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions. PSO is widely used in optimization problems where the solution space is vast and multidimensional.\n",
        "\n",
        "The Particle class represents a single solution in the optimization process. Each particle has its own position and velocity in the solution space."
      ],
      "metadata": {
        "id": "QC3JuQH0XaO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Particle:\n",
        "    \"\"\"\n",
        "    Particle is a neural network representing a potential solution.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_dim, x_range, v_range, r1_range, r2_range, r3_range):\n",
        "        \"\"\"\n",
        "        Particle class constructor\n",
        "        :param num_dim: Number of dimensions.\n",
        "        :param x_range: Range of dimension.\n",
        "        :param v_range: Range of velocity.\n",
        "        \"\"\"\n",
        "\n",
        "        self.x = np.random.uniform(x_range[0], x_range[1], (num_dim, )) # particle position\n",
        "        self.v = np.random.uniform(v_range[0], v_range[1], (num_dim, )) # particle velocity\n",
        "        self.pbest = np.inf                                             # personal best fitness\n",
        "        self.pbestpos = np.zeros((num_dim, ))                           # personal best position\n",
        "        self.informants_best_position = np.zeros((num_dim, ))           # informants best position\n",
        "        self.informants = []                                            # particle's informants\n",
        "        self.r1_range = r1_range                                        # array with min and max range for r1\n",
        "        self.r2_range = r2_range                                        # array with min and max range for r2\n",
        "        self.r3_range = r3_range                                        # array with min and max range for r3\n",
        "\n",
        "    def update_informant_best(self, swarm):\n",
        "        best_fitness = np.inf\n",
        "        for informant in self.informants:\n",
        "            if swarm[informant].pbest < best_fitness:\n",
        "                best_fitness = swarm[informant].pbest\n",
        "                self.informants_best_position = swarm[informant].pbestpos.copy()\n",
        "\n",
        "    def update_velocity(self, global_best_position, alpha, beta, gamma, delta, r1_range, r2_range, r3_range):\n",
        "        r1 = np.random.uniform(self.r1_range[0], self.r1_range[1])\n",
        "        r2 = np.random.uniform(self.r2_range[0], self.r3_range[1])\n",
        "        r3 = np.random.uniform(self.r3_range[0], self.r3_range[1])\n",
        "        # r1, r2, r3 = np.random.rand(3)  # Random coefficients for stochastic components\n",
        "        cognitive_component = beta * r1 * (self.pbestpos - self.x)\n",
        "        social_component = gamma * r2 * (self.informants_best_position - self.x)\n",
        "        global_component = delta * r3 * (global_best_position - self.x)\n",
        "        self.v = alpha * self.v + cognitive_component + social_component + global_component"
      ],
      "metadata": {
        "id": "H5E5uT3ztJGJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class represents a group of particles, working together to explore and optimize a problem space. It initializes with a set number of particles, each having its dimensions, velocities, and inertia weights. Particles communicate amongst each other through informants, allowing them to learn from each other's experiences. The optimize method iteratively updates each particle's position and velocity based on personal and collective experiences, steering the swarm towards the best solution."
      ],
      "metadata": {
        "id": "lZ45quzlXkrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Swarm:\n",
        "    \"\"\"\n",
        "    The Swarm class is a collection of potential solutions, each represented by a particle.\n",
        "    \"\"\"\n",
        "    def __init__(self, no_particle, num_dim, x_range, v_range, iw_range, c, num_informants, r1_range, r2_range, r3_range):\n",
        "        \"\"\"\n",
        "        Swarm class constructor.\n",
        "        :param no_particle:  Number of particles\n",
        "        :param num_dim: Number of dimensions.\n",
        "        :param x_range: Range of dimensions.\n",
        "        :param v_range: Range of velocities.\n",
        "        :param iw_range: Range of interia weights.\n",
        "        :param c: c[0] -> cognitive parameter, c[1] -> social parameter, c[2] -> Global weight, c[3] -> Inertia weight\n",
        "        :param num_informants: Number of informants\n",
        "        \"\"\"\n",
        "        self.p = np.array([Particle(num_dim, x_range, v_range, r1_range, r2_range, r3_range) for i in range(no_particle)])\n",
        "        self.gbest = np.inf\n",
        "        self.gbestpos = np.zeros((num_dim, ))\n",
        "        self.x_range = x_range\n",
        "        self.v_range = v_range\n",
        "        self.iw_range = iw_range\n",
        "        self.c0 = c[0]            # Cognitive weight\n",
        "        self.c1 = c[1]            # Social weight\n",
        "        self.c2 = c[2]            # Global weight\n",
        "        self.c3 = c[3]            # Inertia weight\n",
        "        self.num_dim = num_dim    # Number of dimensions, in this case the total number of weights & biases\n",
        "        self.num_informants = num_informants\n",
        "        self.assign_informants()\n",
        "        self.r1_range = r1_range\n",
        "        self.r2_range = r2_range\n",
        "        self.r3_range = r3_range\n",
        "\n",
        "    def print_informants(self):\n",
        "        \"\"\"\n",
        "        Function prints the informants of every particle.\n",
        "        \"\"\"\n",
        "        for i, particle in enumerate(self.p):\n",
        "            # Print the particle's index and its informants\n",
        "            print(f\"Particle {i} informants: {particle.informants}\")\n",
        "\n",
        "    def assign_informants(self):\n",
        "        \"\"\"\n",
        "        Function assigns informants to every particle\n",
        "        \"\"\"\n",
        "        for i, particle in enumerate(self.p):\n",
        "            informants = set()\n",
        "            # add informants until they are num_informants in informants set\n",
        "            while len(informants) < self.num_informants:\n",
        "                # Randomly select a potential informant\n",
        "                possible_informant = np.random.randint(0, len(self.p))\n",
        "                # if selected informant is not the particle itself, add it to set\n",
        "                if possible_informant != i:\n",
        "                    informants.add(possible_informant)\n",
        "            particle.informants = np.array(list(informants))\n",
        "\n",
        "    def optimize(self, function, X, Y,  print_step,  iter):\n",
        "        \"\"\"\n",
        "        Function used to start optimization.\n",
        "        :param function: function\n",
        "            Function to be optimized\n",
        "        :param X: input data\n",
        "            Used in forward pass.\n",
        "        :param Y: target class\n",
        "            Used to calculate loss.\n",
        "        :param print_step: int\n",
        "            Step for printing\n",
        "        :param iter: int\n",
        "            Number of iterations.\n",
        "        \"\"\"\n",
        "        for i in range(iter):\n",
        "            for particle in self.p:\n",
        "                # print(\"particle.x\",particle.x)\n",
        "                fitness = function(X, Y, particle.x)\n",
        "                # print(\"fitness\",fitness)\n",
        "                # print(\"particle.pbest\",particle.pbest)\n",
        "\n",
        "                if fitness < particle.pbest:\n",
        "                    particle.pbest = fitness\n",
        "                    particle.pbestpos = particle.x.copy()\n",
        "\n",
        "                if fitness < self.gbest:\n",
        "                    self.gbest = fitness\n",
        "                    self.gbestpos = particle.x.copy()\n",
        "\n",
        "            for particle in self.p:\n",
        "                # update informants best\n",
        "                particle.update_informant_best(self.p)\n",
        "                # update particle velocity\n",
        "                particle.update_velocity(self.gbestpos, self.c3, self.c0, self.c1, self.c2, self.r1_range, self.r2_range, self.r3_range)\n",
        "                # update particle position\n",
        "                particle.x = particle.x +  particle.v\n",
        "\n",
        "            # if i % print_step == 0:\n",
        "                # print('iteration#: ', i+1,  ' loss: ', fitness)\n",
        "\n",
        "        #print(\"global best loss: \", self.gbest)\n",
        "\n",
        "    def best_solution(self):\n",
        "        '''\n",
        "        return: array of parameters/weights.\n",
        "        '''\n",
        "        return self.gbestpos"
      ],
      "metadata": {
        "id": "r-Zppi33tVF7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_dim(configuration):\n",
        "  total_weights = 0\n",
        "  total_biases = 0\n",
        "\n",
        "  # The number of nodes in the previous layer, initially the input layer\n",
        "  prev_nodes = configuration[0]\n",
        "\n",
        "  # Loop through each layer (excluding the input layer)\n",
        "  for layer in configuration[1:]:\n",
        "      # Extract the number of nodes in the current layer\n",
        "      nodes = layer[0]\n",
        "\n",
        "      # Calculate weights and biases for the current layer\n",
        "      weights = prev_nodes * nodes\n",
        "      biases = nodes\n",
        "\n",
        "      # Add to total weights and biases\n",
        "      total_weights += weights\n",
        "      total_biases += biases\n",
        "\n",
        "      # Update prev_nodes for the next iteration\n",
        "      prev_nodes = nodes\n",
        "\n",
        "  # Output the total number of dimensions\n",
        "  total_dimensions = total_weights + total_biases\n",
        "  return total_dimensions"
      ],
      "metadata": {
        "id": "_fSlbOplvWPa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the ANN model with manual parameters"
      ],
      "metadata": {
        "id": "6IStMyiKX74b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def ann(config, X_train,  Y_train, no_solution,  w_range, lr_range, iw_range, c , num_informants, loss_function, iterations, r1_range, r2_range, r3_range):\n",
        "def ann(params):\n",
        "    # Unpack all parameters\n",
        "    (\n",
        "        config, X_train, Y_train, no_solution, w_range, lr_range,\n",
        "        iw_range, c, num_informants, loss_function, iterations,\n",
        "        r1_range, r2_range, r3_range\n",
        "    ) = params\n",
        "\n",
        "    # c[0] -> cognitive factor, c[1] -> global factor,  c[2] -> social factor, c[3] -> inertial weight\n",
        "    no_dim = num_dim(config)\n",
        "\n",
        "    s = Swarm(no_solution, no_dim, w_range, lr_range, iw_range, c, num_informants, r1_range, r2_range, r3_range)\n",
        "    # s.print_informants()\n",
        "\n",
        "    # Drop column/s if user picks number of input nodes < number of features\n",
        "    # Pick number of input nodes between 1 and num of features\n",
        "    num_features = X_train.shape[1]  # number of features\n",
        "    number_of_input_nodes = config[0]\n",
        "\n",
        "    if (number_of_input_nodes < num_features):\n",
        "      X_train = X_train.iloc[:, :number_of_input_nodes]\n",
        "\n",
        "    # print(X_train)\n",
        "\n",
        "    # Train:Test Split\n",
        "    test_size = 0.2  # User defined split ratio\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=test_size, random_state=42)\n",
        "\n",
        "    s.optimize(fitness, X_train, Y_train, no_solution, iterations)\n",
        "    W = s.best_solution()\n",
        "    global_best_loss = s.gbest\n",
        "\n",
        "    # Perform forward pass with the best solution (weights)\n",
        "    best_position = unflatten_weights_and_biases(W, config)\n",
        "    best_nn = NeuralNetwork(config, position=best_position)\n",
        "\n",
        "    Y_pred = best_nn.forward(X_train).flatten()\n",
        "    Y_pred2 = best_nn.forward(X_test).flatten()\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    train_accuracy = get_accuracy(Y_train.flatten(), Y_pred)\n",
        "    # # Calculate and print accuracy\n",
        "    test_accuracy = get_accuracy(Y_test.flatten(), Y_pred2)\n",
        "\n",
        "    return train_accuracy, test_accuracy, global_best_loss"
      ],
      "metadata": {
        "id": "wu-kLxdAKCB-"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting"
      ],
      "metadata": {
        "id": "rk7hwGVcYKsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_scores(params, num_runs=10):\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "    global_best_losses = []\n",
        "\n",
        "    # Run the ANN multiple times and record the accuracy and loss\n",
        "    for _ in range(num_runs):\n",
        "        train_accuracy, test_accuracy, global_best_loss = ann(params)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        global_best_losses.append(global_best_loss)\n",
        "\n",
        "    # Calculate the mean accuracy and loss over all runs\n",
        "    mean_train_accuracy = np.mean(train_accuracies)\n",
        "    mean_test_accuracy = np.mean(test_accuracies)\n",
        "    mean_global_best_loss = np.mean(global_best_losses)\n",
        "\n",
        "    return mean_train_accuracy, mean_test_accuracy, mean_global_best_loss"
      ],
      "metadata": {
        "id": "i5Zvkg4XD68G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = [4, [5, sigmoid_activation],[1, sigmoid_activation]]"
      ],
      "metadata": {
        "id": "C00XJF453VGi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with & Optimizing Num of Particles"
      ],
      "metadata": {
        "id": "QmL12VKsEevR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_particles = [\n",
        "    [configuration, x_norm, y, 20, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 25, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 30, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]]\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_particles):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9EHJzgNy_gU",
        "outputId": "bde1964a-7a27-4ab9-b5c9-24bdfcce362b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.3441, Mean Train Accuracy: 0.846, Mean Test Accuracy: 0.828\n",
            "Parameter Set 1: Mean Global Best Loss: 0.1523, Mean Train Accuracy: 0.943, Mean Test Accuracy: 0.929\n",
            "Parameter Set 2: Mean Global Best Loss: 0.1142, Mean Train Accuracy: 0.955, Mean Test Accuracy: 0.943\n",
            "Parameter Set 3: Mean Global Best Loss: 0.0542, Mean Train Accuracy: 0.980, Mean Test Accuracy: 0.977\n",
            "Parameter Set 4: Mean Global Best Loss: 0.0685, Mean Train Accuracy: 0.975, Mean Test Accuracy: 0.966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with & Optimizing number of informants"
      ],
      "metadata": {
        "id": "jteKSs91Eq9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_informants = [\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 6, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 8, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 10, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 13, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 17, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 20, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]]\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_informants):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_I7I5MnE0Z8",
        "outputId": "6cc64272-ab11-4e4e-a63c-4d2739b13ee7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.0794, Mean Train Accuracy: 0.972, Mean Test Accuracy: 0.966\n",
            "Parameter Set 1: Mean Global Best Loss: 0.1124, Mean Train Accuracy: 0.957, Mean Test Accuracy: 0.945\n",
            "Parameter Set 2: Mean Global Best Loss: 0.1752, Mean Train Accuracy: 0.935, Mean Test Accuracy: 0.926\n",
            "Parameter Set 3: Mean Global Best Loss: 0.2390, Mean Train Accuracy: 0.914, Mean Test Accuracy: 0.892\n",
            "Parameter Set 4: Mean Global Best Loss: 0.1935, Mean Train Accuracy: 0.926, Mean Test Accuracy: 0.911\n",
            "Parameter Set 5: Mean Global Best Loss: 0.2906, Mean Train Accuracy: 0.874, Mean Test Accuracy: 0.862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with & Optimizing number of Particles & Informants"
      ],
      "metadata": {
        "id": "nXoAzz-tEq6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_particles_informants = [\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 6, BinaryCrossEntropy(), 100, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 10, BinaryCrossEntropy(), 100, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 100, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 25, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 3, BinaryCrossEntropy(), 100, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 20, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 3, BinaryCrossEntropy(), 100, [0,1], [0,1], [0,1]]\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_particles_informants):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDPjjdDPE1KA",
        "outputId": "1858a43b-0c82-485c-9268-d598d2d51af2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.3305, Mean Train Accuracy: 0.844, Mean Test Accuracy: 0.840\n",
            "Parameter Set 1: Mean Global Best Loss: 0.2773, Mean Train Accuracy: 0.880, Mean Test Accuracy: 0.867\n",
            "Parameter Set 2: Mean Global Best Loss: 0.2693, Mean Train Accuracy: 0.870, Mean Test Accuracy: 0.855\n",
            "Parameter Set 3: Mean Global Best Loss: 0.4571, Mean Train Accuracy: 0.764, Mean Test Accuracy: 0.751\n",
            "Parameter Set 4: Mean Global Best Loss: 0.4315, Mean Train Accuracy: 0.773, Mean Test Accuracy: 0.748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase the number of Particles and increase & decrease global weight"
      ],
      "metadata": {
        "id": "cajD2GvcEq3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_particles_glob = [\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.3, 0.2), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.3, 0.5), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.3, 1.2), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.3, 2), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.3, 3), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]]\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_particles_glob):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyCufmvSE2BQ",
        "outputId": "a2437688-625e-4696-ab8f-a631b30a8386"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.9202, Mean Train Accuracy: 0.445, Mean Test Accuracy: 0.423\n",
            "Parameter Set 1: Mean Global Best Loss: 0.6833, Mean Train Accuracy: 0.555, Mean Test Accuracy: 0.577\n",
            "Parameter Set 2: Mean Global Best Loss: 0.4971, Mean Train Accuracy: 0.778, Mean Test Accuracy: 0.765\n",
            "Parameter Set 3: Mean Global Best Loss: 0.6222, Mean Train Accuracy: 0.680, Mean Test Accuracy: 0.691\n",
            "Parameter Set 4: Mean Global Best Loss: 0.8853, Mean Train Accuracy: 0.445, Mean Test Accuracy: 0.423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase number of particles and informants, then give the social weight more priority."
      ],
      "metadata": {
        "id": "b6WN1YpTEqyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_particles_more_social = [\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 1.0, 0.3), 6, BinaryCrossEntropy(), 120, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.4, 0.8, 0.2), 9, BinaryCrossEntropy(), 120, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.2, 0.8, 0.4), 12, BinaryCrossEntropy(), 120, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.5, 0.5, 0.5), 16, BinaryCrossEntropy(), 120, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.5, 0.3, 0.5), 20, BinaryCrossEntropy(), 120, [0,1], [0,1], [0,1]]\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_particles_more_social):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIge5Uq7E2yb",
        "outputId": "e9fa1ee5-26cc-46d3-b491-9353055efeec"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.6853, Mean Train Accuracy: 0.555, Mean Test Accuracy: 0.577\n",
            "Parameter Set 1: Mean Global Best Loss: 0.7379, Mean Train Accuracy: 0.482, Mean Test Accuracy: 0.477\n",
            "Parameter Set 2: Mean Global Best Loss: 0.6788, Mean Train Accuracy: 0.555, Mean Test Accuracy: 0.577\n",
            "Parameter Set 3: Mean Global Best Loss: 0.6761, Mean Train Accuracy: 0.556, Mean Test Accuracy: 0.577\n",
            "Parameter Set 4: Mean Global Best Loss: 0.6774, Mean Train Accuracy: 0.556, Mean Test Accuracy: 0.577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_particles_more_social2 = [\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 1.0, 0.3), 6, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.4, 0.8, 0.2), 9, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.2, 0.8, 0.4), 12, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.5, 0.5, 0.5), 16, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "    [configuration, x_norm, y, 40, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.5, 0.3, 0.5), 20, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]]\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_particles_more_social2):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "HmBxirAWfJmj",
        "outputId": "0af68624-0e77-4a76-bd6c-829724f0bd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.6811, Mean Train Accuracy: 0.555, Mean Test Accuracy: 0.577\n",
            "Parameter Set 1: Mean Global Best Loss: 0.7277, Mean Train Accuracy: 0.478, Mean Test Accuracy: 0.469\n",
            "Parameter Set 2: Mean Global Best Loss: 0.6823, Mean Train Accuracy: 0.555, Mean Test Accuracy: 0.577\n",
            "Parameter Set 3: Mean Global Best Loss: 0.6702, Mean Train Accuracy: 0.580, Mean Test Accuracy: 0.605\n",
            "Parameter Set 4: Mean Global Best Loss: 0.6801, Mean Train Accuracy: 0.555, Mean Test Accuracy: 0.577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test: Train Split Ratio"
      ],
      "metadata": {
        "id": "4ybBFRHyEqvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_num_particles_split = [\n",
        "    [configuration, x_norm, y, 35, (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.5, 0.3, 0.2, 0.9), 4, BinaryCrossEntropy(), 200, [0,1], [0,1], [0,1]],\n",
        "]\n",
        "\n",
        "for i, params in enumerate(params_num_particles_split):\n",
        "    mean_train_accuracy, mean_test_accuracy, mean_global_best_loss = calculate_mean_scores(params)\n",
        "    print(f\"Parameter Set {i}: Mean Global Best Loss: {mean_global_best_loss:.4f}, \"\n",
        "          f\"Mean Train Accuracy: {mean_train_accuracy:.3f}, Mean Test Accuracy: {mean_test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMHw1q9tE3h5",
        "outputId": "a404d3d8-7e46-45ca-e3e0-b727335dff90"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Set 0: Mean Global Best Loss: 0.1200, Mean Train Accuracy: 0.954, Mean Test Accuracy: 0.944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import itertools\n",
        "\n",
        "# def generate_configurations(min_layers, max_layers, min_nodes, max_nodes):\n",
        "#     configurations = []\n",
        "#     for num_layers in range(min_layers, max_layers + 1):\n",
        "#         for nodes_per_layer in itertools.product(range(min_nodes, max_nodes + 1), repeat=num_layers):\n",
        "#             configurations.append(nodes_per_layer)\n",
        "#     return configurations\n",
        "\n",
        "# # Example user inputs\n",
        "# min_layers = 1  # User-defined minimum number of hidden layers\n",
        "# max_layers = 3  # User-defined maximum number of hidden layers\n",
        "# min_nodes = 3   # User-defined minimum number of nodes per layer\n",
        "# max_nodes = 5   # User-defined maximum number of nodes per layer\n",
        "\n",
        "# # Generate configurations\n",
        "# configurations = generate_configurations(min_layers, max_layers, min_nodes, max_nodes)\n",
        "\n",
        "# results = []\n",
        "# for config in configurations:\n",
        "#     # Assuming all hidden layers use relu_activation for simplicity\n",
        "#     full_config = [input_nodes] + [[nodes, relu_activation] for nodes in config] + [[1, sigmoid_activation]]\n",
        "#     accuracies = ann(full_config, x_norm, y, x_test, y_test)\n",
        "#     train_acc = accuracies[0]\n",
        "#     test_acc = accuracies[1]\n",
        "#     results.append((config, train_acc, test_acc))\n",
        "\n",
        "# # Find the best configuration\n",
        "# best_config = max(results, key=lambda x: x[2])  # Select based on test accuracy\n",
        "\n",
        "# print(f\"Best Configuration: {best_config[0]}\")\n",
        "# print(f\"Train Accuracy: {best_config[1]}\")\n",
        "# print(f\"Test Accuracy: {best_config[2]}\")\n",
        "\n",
        "# # Plotting results\n",
        "# train_accuracies = [result[1] for result in results]\n",
        "# test_accuracies = [result[2] for result in results]\n",
        "# labels = [str(config[0]) for config in results]\n",
        "\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(labels, train_accuracies, label='Train Accuracy')\n",
        "# plt.plot(labels, test_accuracies, label='Test Accuracy')\n",
        "# plt.xlabel('Configurations')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.title('ANN Configurations vs. Accuracy')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "8EujulCNYM3p"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xH7_hOGzrmiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}